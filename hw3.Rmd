---
title: "Homework3"
author: "Chen Zhang"
date: "3/14/2019"
output: word_document
---
### 1.Consider again the Bouguer gravity anomaly data set in Olea (2006).
```{r}
library(geoR)
#obtain data
data = readxl::read_excel("data.xlsx")
data = data[,c(2:4)]  #delete the ID column
names(data)=c("x","y","z")  #rename variable as "x","y","z"
```
####(a) Based on the discussion in class, consider re–analyzing the data set to arrive at a tentative model(it may end up being the same one you arrived at in homework 2).  

Frist, we could do exploratory data analysis.  
It is clearly to see that the mean function is not constant. In this case, we need to estimate the mean function. I think the possible mean function could be $f(x)=x+x^2+y+xy+x^2y+xy^2$
```{r}
lm.model = lm(z~x+y,data=data)  # run a linear regression
poly.model = lm(z~x+I(x^2)+y+x*y+I(x^2*y)+I(x*y^2),data = data) 
# add polynomial term. The original formula is poly.model = lm(z~x+I(x^2)+I(y^2)+y+x*y+I(x^2*y)+I(x*y^2),data = data). After summary the model, I found that y^2 is not significant, so I delete this term. 
```

Becuase the mean function is not constant, we need to use residual to estimate the semivariogram function. 
```{r,fig.width=10,fig.height=5}
data.var = data
data.var$z = poly.model$residuals 
#because the mean function is not constant, we need to work on residuals.
data.var = as.geodata(data.var) 
#replace original z using the residual as z
breaks = seq(0,30)
par(mfrow=c(1,2))
plot(variog(data.var, estimator.type = "modulus", breaks=breaks),main="variogram function estimation using residuals")
plot(variog(data.var, estimator.type = "classical", breaks=breaks),main="variogram function estimation using residuals")
#I use the residual to estimate the semivariogram function with different estimator type
```

Based on the graph, we could fit the model to find a best family. 
```{r}
gr.rvar = variog(data.var,breaks = breaks)

#to fit the model, we need to calcualte the uniroot. 
#based on the graph, we can estimate that the nuggest is 0, sill is 0.45,range is 20,effective range is 0.95x0.45=0.4275
g = function(x) 0.45*(1 - (20/x) *besselK(20/x,nu=1)) + 0 -0.4275
uniroot(g,c(0.1,20)) $root
```

Based on the resultm we can know that the uniroot is 5. 
```{r,fig.width=10,fig.height=5}
# fit the model using exp
gr.fit.e = variofit(vario =gr.rvar,ini.cov.pars = c(0.45,5),cov.model = "exp",nugget = 0,weights = "cressie" ) 

#fit the model using spherical
gr.fit.w = variofit(vario =gr.rvar,ini.cov.pars = c(0.45,5),cov.model = "wave",nugget = 0,weights = "cressie" ) 

par(mfrow=c(1,2))
plot(gr.rvar ,main="exp") #plot the fitted model using matern
lines.variomodel(cov.model="matern",cov.pars = gr.fit.e$cov.pars,nugget =gr.fit.e$nugget,col="blue",kappa=1,lwd=3,max.dist = 32 )

plot(gr.rvar ,main="wave") #plot the fitted model using wave
lines.variomodel(cov.model="wave",cov.pars = gr.fit.w$cov.pars,nugget =gr.fit.w$nugget,col="blue",kappa=1,lwd=3,max.dist = 32 )
```

Since I fit two models, then we could compare those two models. 
```{r}
#compare the covariance parameter using two different, and choose the better one. 
com.model = data.frame("model"=c("exp","wave"),"nugget"=c(gr.fit.e$nugget,gr.fit.w$nugget),"sigma"=c(gr.fit.e$cov.pars[1],gr.fit.w$cov.pars[1]),"phi"=c(gr.fit.e$cov.pars[2],gr.fit.w$cov.pars[2]))
com.model
```

Based on the graph, it seems like "wave" model fit the points better. 

#### (b)Fit the model by maximum likelihood and by restricted maximum likelihood. Compare the different mean and covariance parameter estimates obtained by least squares, maximum likelihood and restricted maximum likelihood.

```{r,warning=FALSE}
#we use ML to obtain the parameter estimates. Instead of working on the residuals, ML uses a different way. 

gr.ml =likfit(as.geodata(data),cov.model = "exp",ini=c(1,32),fix.nugget = T,nugget = 0,lik.method = "ML",trend="2nd")  #fit the model using ML
gr.ml$beta

gr.rml = likfit(as.geodata(data),cov.model = "exp",ini=c(1,32),fix.nugget = T,nugget = 0,lik.method = "RML",trend = "2nd") #fit the model using RML
```

On (a), we work on the residuals and I estimate a mean function. The general form of mean function is $\mu(s)= \sum_{j=1}^{n} \beta_jf_j(s)$, based on the polynomial model, we could have estimated $\beta $ which is `r sum(poly.model$coefficients)`, then we could compare the estimated $\beta, \sigma^2,\theta.$ using exp model. 
```{r,echo=FALSE}
com.esti = data.frame("method"=c("WLS","ML","RML"),"beta"=c(sum(poly.model$coefficients),sum(gr.ml$beta),sum(gr.rml$beta)),"sigmasq" = c(gr.fit.e$cov.pars[1]^2,gr.ml$sigmasq,gr.rml$sigmasq),"phi"=c(gr.fit.e$cov.pars[2],gr.ml$phi,gr.rml$phi))
com.esti

```

Based on the result, we can see that the $\beta$ is not that big difference. which means the mean function is very close. For the WLS, the mean function is $\mu(x)=\beta_0+x\beta_1+x^2\beta_2+y\beta_3+xy\beta_4+x^2y\beta_5+xy^2\beta_6$, $\sigma^2$ and $\theta$, for the ML and REML, the mean function is $\mu(x)= \beta0 + \beta_1x + \beta_2y + \beta_3x^2 + \beta_4y^2 + \beta_5xy$. Then we could summary all $\beta_j$, we have $\beta$ is not that big difference. $\sigma^2$ for WLS and ML 
has smaller differences. REML has the largest  $\sigma ^2$ and $\theta$, not close to WLS and ML.   


#### (c) Display in the same plot the empirical semivariogram, as well as the semivariograms fitted by weighted least squares, maximum likelihood and restricted maximum likelihood. Discuss similarities and differences.

```{r}
plot(variog(data.var, estimator.type = "modulus", breaks=breaks),main="comparsion of WLS,ML,and RML fitted semivariogram",ylim=c(0,0.8))
lines(gr.fit.e,col="blue",lty=2,lwd=3)
lines(gr.ml,col="red",lty=3,lwd=3)
lines(gr.rml,lwd=1.5,col="green")
legend(1,0.8,c("method","WLS","ML","RML"),lty=0:3,bty="n")
```

Based on the graph, we can see that there is no such big difference between maximum likelihood and restricted maximum likelihood. ML and RML are similar to each other, but have large difference with weighted least squares. When distance is smaller than 10, RML and ML fitted semivariogram much better than weighted least squares. With distance goes larger, weighted least squares still fit, but ML and RML go beyond the data points. 

### 2 This problem seeks to investigate by simulation some of the properties of the sampling distributions of maximum likelihood (ML) and restricted maximum likelihood (REML) estimators of covariance parameters.

Let $Z(·)$ be a Gaussian random field with mean function $\mu(s)$ and isotropic covariance function $C(r) = \sigma ^2 exp(−r/θ)$. Consider the sampling design $S_n$ in D = [0, 1] × [0, 1] displayed in Figure 1(a) of Chapter 4.

#### (a)  For the above model suppose $\mu(s) = β_0 = 0, \sigma ^2 = 1 and \theta = 0.1 or 0.3$. Simulate 500 realizations of $Z(·)$ at Sn with these two sets of parameters. From each simulated data set compute the ML and REML estimates of $(\beta_0, \sigma ^2,\theta).$ Then, compare their respective sampling distributions, biases, variances and mean squared errors. Discuss the findings.

In this question, we know that the mean function is 0 and it is constant. 

```{r}
#simulate the 500 data using theta = 0.1
x1 = seq(0,1,length=15)
dsgn1 = expand.grid(x1,x1)
names(dsgn1) = c("x","y")
# we know that variance = 1, and mean = 0
n.star = 500
sim.data1 = grf(n=nrow(grid),grid = dsgn1,nsim=n.star,cov.model = "exp",cov.pars = c(1,0.1))

#theta = 0,1, using ML
beta1=NULL;sigma1=NULL;phi1=NULL
for (i in 1:n.star) {
  dsgn1$z = sim.data1$data[,i]
  ml.1 = likfit(as.geodata(dsgn1),cov.model = "exp",ini=c(1,32),fix.nugget = T,nugget = 0,lik.method = "ML") 
  beta1[i] = ml.1$beta
  sigma1[i] = ml.1$sigmasq
  phi1[i] = ml.1$phi
}

#theta = 0,1, using rML
beta1.1=NULL;sigma1.1=NULL;phi1.1=NULL
for (i in 1:n.star) {
  dsgn1$z = sim.data1$data[,i]
  ml.11 = likfit(as.geodata(dsgn1),cov.model = "exp",ini=c(1,32),fix.nugget = T,nugget = 0,lik.method = "RML") 
  beta1.1[i] = ml.11$beta
  sigma1.1[i] = ml.11$sigmasq
  phi1.1[i] = ml.11$phi
}


#simulate the 500 data using theta = 0.3
sim.data2 = grf(n=nrow(grid),grid = dsgn1,nsim=n.star,cov.model = "exp",cov.pars = c(1,0.3))

#using ML
beta1.3=NULL;sigma1.3=NULL;phi1.3=NULL
for (i in 1:n.star) {
  dsgn1$z = sim.data2$data[,i]
  ml.13 = likfit(as.geodata(dsgn1),cov.model = "exp",ini=c(1,32),fix.nugget = T,nugget = 0,lik.method = "ML") 
  beta1.3[i] = ml.13$beta
  sigma1.3[i] = ml.13$sigmasq
  phi1.3[i] = ml.13$phi
}

#using RML
beta2=NULL;sigma2=NULL;phi2=NULL
for (i in 1:n.star) {
  dsgn1$z = sim.data2$data[,i]
  rml.1 = likfit(as.geodata(dsgn1),cov.model = "exp",ini=c(1,32),fix.nugget = T,nugget = 0,lik.method = "RML") 
    beta2[i] = rml.1$beta
   sigma2[i] = rml.1$sigmasq
   phi2[i] = rml.1$phi
}

```

Now, we could compute the parameters.
```{r}
para1 = data.frame(beta1,sigma1,phi1);summary(para1) 
#when theta = 0.1, the parameter obtained by ML 

para1.1 = data.frame(beta1.1,sigma1.1,phi1.1);summary(para1.1)
#when theta = 0.1, the parameter obtained by RML 

para2 = data.frame(beta2,sigma2,phi2);summary(para2)
#when theta = 0.3, the parameter obtained by RML 
para1.3 = data.frame(beta1.3,sigma1.3,phi1.3);summary(para1.3)
#when theta = 0.3, the parameter obtained by ML 

```


Since we run the data, now, we could compare the sampling distributions, biases, variances and mean squared errors. Discuss the findings.   
First, we could compare the sampling distribution, we could compare the mean of each parametors. Meanwhile, we could also calculate the bias, variance, and mse based on the formula. 
```{r,echo=FALSE,fig.height=10}
#sample distribution
beta1.s = mean(beta1); beta1.1s = mean(beta1.1)
beta1.3s = mean(beta1.3);beta2.s = mean(beta2)


sigma1.s = mean(sigma1); sigma1.1s = mean(sigma1.1)
sigma1.3s = mean(sigma1.3);sigma2.s = mean(sigma2)

phi1.s = mean(phi1); phi1.1s = mean(phi1.1)
phi1.3s = mean(phi1.3);phi2.s = mean(phi2)

#bias
beta1.b = beta1.s - 0.1 ; beta1.1b = beta1.1s- 0.1
beta1.3b = beta1.3s - 0.3 ;beta2.b = beta2.s - 0.3

sigma1.b = sigma1.s - 0.1 ; sigma1.1b = sigma1.1s - 0.1
sigma1.3b = sigma1.3s -0.3; sigma2.b = sigma2.s - 0.3


phi1.b = phi1.s - 0.1 ; phi1.1b = phi1.1s-0.1
phi1.3b = phi1.3s- 0.3 ; phi2.b = phi2.s - 0.3

#variance
beta1.v = mean((beta1-beta1.s)^2); beta1.1v = mean((beta1.1-beta1.1s)^2)
beta1.3v = mean((beta1.3-beta1.3s)^2);beta2.v = mean((beta2-beta2.s)^2)

sigma1.v = mean((sigma1-sigma1.s)^2); sigma1.1v = mean((sigma1.1-sigma1.1s)^2);
sigma1.3v = mean((sigma1.3-sigma1.3s)^2);sigma2.v = mean((sigma2-sigma2.s)^2)

phi1.v = mean((phi1-phi1.s)^2); phi1.1v = mean((phi1.1-phi1.1s)^2)
phi1.3v = mean((phi1.3-phi1.3s)^2);phi2.v = mean((phi2-phi2.s)^2)

#mse
beta1.mse = beta1.b^2 + beta1.v ;beta1.1mse = beta1.1b^2 + beta1.1v 
beta1.3mse = beta1.3b^2 + beta1.3v ; beta2.mse = beta2.b^2 + beta2.v 

sigma1.mse = sigma1.b^2 + sigma1.v ; sigma1.1mse = sigma1.1b^2 + sigma1.1v
sigma1.3mse = sigma1.3b^2 + sigma1.3v ; sigma2.mse = sigma2.b^2 + sigma2.v

phi1.mse = phi1.b^2 + phi1.v ;phi1.1mse = phi1.1b^2 + phi1.1v
phi1.3mse = phi1.3b^2 + phi1.3v; phi2.mse = phi2.b^2 + phi2.v

#compare the sampling distribution
par(mfrow=c(3,4))
hist(beta1,main="beta obtained by ML theta=0.1")
hist(beta1.1,main="beta obtained by RML theta=0.1")
hist(beta1.3,main="beta obtained by EML theta=0.3")
hist(beta2,main = "beta obtained by REML theta=0.3")

hist(sigma1,main="sigma obtained by ML theta = 0.1")
hist(sigma1.1,main="sigma obtained by RML theta=0.1")
hist(sigma1.3 ,main="sigma obtained by ML theta=0.3")
hist(sigma2,main="sigma obtained by REML theta=0.3")

hist(phi1,main="phi obtained by ML theta=0.1")
hist(phi1.1,main="phi obtained by RML theta=0.1")
hist(phi1.3,main="phi obtained by ML theta=0.3")
hist(phi2,main="phi obtained by REML theta = 0.3")


com.beta1 = data.frame("method(beta)"=c("ML","REML","ML","REML"),"theta"=c(0.1,0.1,0.3,0.3), "bias"=c(beta1.b,beta1.1b,beta1.3b,beta2.b),"variance"=c(beta1.v,beta1.1v,beta1.3v,beta2.v),"mse"=c(beta1.mse,beta1.1mse,beta1.3mse,beta2.mse)) ;com.beta1

com.sigma1 = data.frame("method(sigma)"=c("ML","REML","ML","REML"),"theta"=c(0.1,0.1,0.3,0.3),"bias"=c(sigma1.b,sigma1.1b,sigma1.3b,sigma2.b),"variance"=c(sigma1.v,sigma1.1v,sigma1.3v,sigma2.v),"mse"=c(sigma1.mse,sigma1.1mse,sigma1.3mse,sigma2.mse)) ;com.sigma1

com.phi1 = data.frame("method(phi)"=c("ML","REML","ML","REML"),"theta"=c(0.1,0.1,0.3,0.3),"bias"=c(phi1.b,phi1.1b,phi1.3b,phi2.b),"variance"=c(phi1.v,phi1.1v,phi1.3v,phi2.v),"mse"=c(phi1.mse,phi1.1mse,phi1.3mse,phi2.mse)) ;com.phi1

```

When $\theta=0.1 or \thera=0.3$, the bias, variance and mse of $\beta$ obtained by ML is no big difference by REML. $\theta=0.1$ the bias, variance and mse of $sigma^2$ obtained by  ML is slightly smaller than REML. But the difference is not that obvious. The same when $\theta=0.3$. While, for the same method, such as ML, the $theta$ plays an important role. The bias, variance and mse of ML is different when $=0.1$ compare to $theta=0.3$. None of the sampling distribution are normal distribution. 

#### (b) Suppose now that $Z(·)$ has mean function $\mu(s) = \beta_0 + \beta_1x + \beta_2y$ ,with s = (x, y). Suppose that $\beta_0 = 0, \beta_1 = 0.5, \beta_2 = 1, \sigma^2 = 1 and \theta = 0.1 or 0.3.$ Simulate 500 realizations of $Z(·)$ at $S_n$ with these two sets of parameters. From each simulated data set compute the ML and REML estimates of $(\beta_0, \beta_1, \beta_2, \sigma^2, \theta).$ Then, compare their respective sampling distributions, biases, variances and mean squared errors. Discuss the findings.

```{r}
#the mean function is o.5x+y, variance = 1
# first, we simulate the data using theta = 0.1
sim.data3 = grf(n=nrow(grid),grid = dsgn1,nsim=n.star,cov.model = "exp",cov.pars = c(1,0.1),mean=0.5*dsgn1$x+dsgn1$y)

#simulate data 500 times using ML
beta3=NULL;sigma3=NULL;phi3=NULL
for (i in 1:n.star) {
  dsgn1$z = sim.data3$data[,i]
  ml.2 = likfit(as.geodata(dsgn1),cov.model = "exp",ini=c(1,32),fix.nugget = T,nugget = 0,lik.method = "ML") 
  beta3[i] = ml.2$beta
  sigma3[i] = ml.2$sigmasq
  phi3[i] = ml.2$phi
}

#simulate data 500 times using RML
beta3.1=NULL;sigma3.1=NULL;phi3.1=NULL
for (i in 1:n.star) {
  dsgn1$z = sim.data3$data[,i]
  ml.2.1 = likfit(as.geodata(dsgn1),cov.model = "exp",ini=c(1,32),fix.nugget = T,nugget = 0,lik.method = "RML") 
  beta3.1[i] = ml.2.1$beta
  sigma3.1[i] = ml.2.1$sigmasq
  phi3.1[i] = ml.2.1$phi
}

#simulate the 500 data using theta = 0.3
sim.data4 = grf(n=nrow(grid),grid = dsgn1,nsim=n.star,cov.model = "exp",cov.pars = c(1,0.3),mean=0.5*dsgn1$x+dsgn1$y)

beta3.3=NULL;sigma3.3=NULL;phi3.3=NULL
for (i in 1:n.star) {
  dsgn1$z = sim.data4$data[,i]
  ml.2.3 = likfit(as.geodata(dsgn1),cov.model = "exp",ini=c(1,32),fix.nugget = T,nugget = 0,lik.method = "ML") 
  beta3.3[i] = ml.2.3$beta
  sigma3.3[i] = ml.2.3$sigmasq
  phi3.3[i] = ml.2.3$phi
}

beta4=NULL;sigma4=NULL;phi4=NULL
for (i in 1:n.star) {
  dsgn1$z = sim.data4$data[,i]
  rml.2 = likfit(as.geodata(dsgn1),cov.model = "exp",ini=c(1,32),fix.nugget = T,nugget = 0,lik.method = "RML") 
    beta4[i] = rml.2$beta
   sigma4[i] = rml.2$sigmasq
   phi4[i] = rml.2$phi
}

```

```{r}
#compute the parameters
para3 = data.frame(beta3,sigma3,phi3);summary(para3) 
#when theta = 0.1, the parameter obtained by ML 

para3.1 = data.frame(beta3.1,sigma3.1,phi3.1);summary(para3.1)
#when theta = 0.1, the parameter obtained by RML 

para4 = data.frame(beta4,sigma4,phi4);summary(para4)
#when theta = 0.3, the parameter obtained by RML 
para3.3 = data.frame(beta3.3,sigma3.3,phi3.3);summary(para3.3)
#when theta = 0.3, the parameter obtained by ML 

```


```{r,fig.height=10,echo=FALSE}

#sample distribution
beta3.s = mean(beta3); beta3.1s = mean(beta3.1)
beta3.3s = mean(beta3.3);beta4.s = mean(beta4)


sigma3.s = mean(sigma3); sigma3.1s = mean(sigma3.1)
sigma3.3s = mean(sigma3.3);sigma4.s = mean(sigma4)

phi3.s = mean(phi3); phi3.1s = mean(phi3.1)
phi3.3s = mean(phi3.3);phi4.s = mean(phi4)

#bias
beta3.b = beta3.s - 0.1 ; beta3.1b = beta3.1s- 0.1
beta3.3b = beta3.3s - 0.3 ;beta4.b = beta4.s - 0.3

sigma3.b = sigma3.s - 0.1 ; sigma3.1b = sigma3.1s - 0.1
sigma3.3b = sigma3.3s -0.3; sigma4.b = sigma4.s - 0.3


phi3.b = phi3.s - 0.1 ; phi3.1b = phi3.1s-0.1
phi3.3b = phi3.3s- 0.3 ; phi4.b = phi4.s - 0.3

#variance
beta3.v = mean((beta3-beta3.s)^4); beta3.1v = mean((beta3.1-beta3.1s)^4)
beta3.3v = mean((beta3.3-beta3.3s)^4);beta4.v = mean((beta4-beta4.s)^4)

sigma3.v = mean((sigma3-sigma3.s)^4); sigma3.1v = mean((sigma3.1-sigma3.1s)^4);
sigma3.3v = mean((sigma3.3-sigma3.3s)^4);sigma4.v = mean((sigma4-sigma4.s)^4)

phi3.v = mean((phi3-phi3.s)^4); phi3.1v = mean((phi3.1-phi3.1s)^4)
phi3.3v = mean((phi3.3-phi3.3s)^4);phi4.v = mean((phi4-phi4.s)^4)

#mse
beta3.mse = beta3.b^4 + beta3.v ;beta3.1mse = beta3.1b^4 + beta3.1v 
beta3.3mse = beta3.3b^4 + beta3.3v ; beta4.mse = beta4.b^4 + beta4.v 

sigma3.mse = sigma3.b^4 + sigma3.v ; sigma3.1mse = sigma3.1b^4 + sigma3.1v
sigma3.3mse = sigma3.3b^4 + sigma3.3v ; sigma4.mse = sigma4.b^4 + sigma4.v

phi3.mse = phi3.b^4 + phi3.v ;phi3.1mse = phi3.1b^4 + phi3.1v
phi3.3mse = phi3.3b^4 + phi3.3v; phi4.mse = phi4.b^4 + phi4.v

#compare the sampling distribution
par(mfrow=c(3,4))
hist(beta3,main="beta obtained by ML theta=0.1")
hist(beta3.1,main="beta obtained by RML theta=0.1")
hist(beta3.3,main="beta obtained by ML theta=0.3")
hist(beta4,main = "beta obtained by REML theta=0.3")

hist(sigma3,main="sigma obtained by ML theta = 0.1")
hist(sigma3.1,main="sigma obtained by RML theta=0.1")
hist(sigma3.3 ,main="sigma obtained by ML theta=0.3")
hist(sigma4,main="sigma obtained by REML theta=0.3")

hist(phi3,main="phi obtained by ML theta=0.1")
hist(phi3.1,main="phi obtained by RML theta=0.1")
hist(phi3.3,main="phi obtained by ML theta=0.3")
hist(phi4,main="phi obtained by REML theta = 0.3")


com.beta3 = data.frame("method(beta)"=c("ML","REML","ML","REML"),"theta"=c(0.1,0.1,0.3,0.3), "bias"=c(beta3.b,beta3.1b,beta3.3b,beta4.b),"variance"=c(beta3.v,beta3.1v,beta3.3v,beta4.v),"mse"=c(beta3.mse,beta3.1mse,beta3.3mse,beta4.mse)) ;com.beta3

com.sigma3 = data.frame("method(sigma)"=c("ML","REML","ML","REML"),"theta"=c(0.1,0.1,0.3,0.3),"bias"=c(sigma3.b,sigma3.1b,sigma3.3b,sigma4.b),"variance"=c(sigma3.v,sigma3.1v,sigma3.3v,sigma4.v),"mse"=c(sigma3.mse,sigma3.1mse,sigma3.3mse,sigma4.mse)) ;com.sigma3

com.phi3 = data.frame("method(phi)"=c("ML","REML","ML","REML"),"theta"=c(0.1,0.1,0.3,0.3),"bias"=c(phi3.b,phi3.1b,phi3.3b,phi4.b),"variance"=c(phi3.v,phi3.1v,phi3.3v,phi4.v),"mse"=c(phi3.mse,phi3.1mse,phi3.3mse,phi4.mse)) ;com.phi3


```

When $\theta=0.1 or \thera=0.3$, the bias, variance and mse of $\beta$ obtained by ML is no big difference by REML. $\theta=0.1$ the bias, variance and mse of $sigma^2$ obtained by  ML is slightly smaller than REML. But the difference is not that obvious. The same when $\theta=0.3$. While, for the same method, such as ML, the $theta$ plays an important role. The bias, variance and mse of ML is different when $=0.1$ compare to $theta=0.3$. None of the sampling distribution are normal distribution.  

### (c)  Formulate a question you consider relevant in regard to the sampling distribution of the ML stimators of the covariance parameters. Then obtain and answer using simulation.

In this case, we could assume that mean function is only related with x and has nothing to do with y. So the mean function is $\mu(s)=\beta_0+\beta_1x+\beta_2y$,$\beta_0=\beta_2=0,\beta=1$, variance = 1, $\theta=0.1$

```{r}
sim.data5 = grf(n=nrow(grid),grid = dsgn1,nsim=n.star,cov.model = "exp",cov.pars = c(1,0.1),mean=dsgn1$x)

#simulate data 500 times
beta5=NULL;sigma5=NULL;phi5=NULL
for (i in 1:n.star) {
  dsgn1$z = sim.data5$data[,i]
  ml.3 = likfit(as.geodata(dsgn1),cov.model = "exp",ini=c(1,32),fix.nugget = T,nugget = 0,lik.method = "ML") 
  beta5[i] = ml.3$beta
  sigma5[i] = ml.3$sigmasq
  phi5[i] = ml.3$phi
}


beta6=NULL;sigma6=NULL;phi6=NULL
for (i in 1:n.star) {
  dsgn1$z = sim.data5$data[,i]
  rml.3 = likfit(as.geodata(dsgn1),cov.model = "exp",ini=c(1,32),fix.nugget = T,nugget = 0,lik.method = "RML") 
    beta6[i] = rml.3$beta
   sigma6[i] = rml.3$sigmasq
   phi6[i] = rml.3$phi
}
```

```{r}
para5 = data.frame(beta5,sigma5,phi5)
summary(para5)
para6 = data.frame(beta6,sigma6,phi6)
summary(para6)
```


```{r,fig.height=10,echo=FALSE}
beta5.s = mean(beta5);beta6.s = mean(beta6)
sigma5.s = mean(sigma5);sigma6.s = mean(sigma6)
phi5.s = mean(phi5);phi6.s = mean(phi6)

#bias
beta5.b = beta5.s - 0.1 ; beta6.b = beta6.s - 0.5
sigma5.b = sigma5.s - 0.1 ; sigma6.b = sigma6.s - 0.5
phi5.b = phi5.s - 0.1 ; phi6.b = phi6.s - 0.5

#variance
beta5.v = mean((beta5-beta5.s)^2); beta6.v = mean((beta6-beta6.s)^2)
sigma5.v = mean((sigma5-sigma5.s)^2); sigma6.v = mean((sigma6-sigma6.s)^2)
phi5.v = mean((phi5-phi5.s)^2); phi6.v = mean((phi6-phi6.s)^2)

#mse
beta5.mse = beta5.b^2 + beta5.v ; beta6.mse = beta6.b^2 + beta6.v 
sigma5.mse = sigma5.b^2 + sigma5.v ; sigma6.mse = sigma6.b^2 + sigma6.v
phi5.mse = phi5.b^2 + phi5.v ; phi6.mse = phi6.b^2 + phi6.v

#compare the sampling distribution
par(mfrow=c(3,2))
hist(beta5,main="beta obtained by ML")
hist(beta6,main = "beta obtained by REML")
hist(sigma5,main="sigma obtained by ML")
hist(sigma6,main="sigma obtained by REML")
hist(phi5,main="phi obtained by ML")
hist(phi6,main="phi obtained by REML")


com.beta5 = data.frame("method(beta)"=c("ML","REML"),"bias"=c(beta5.b,beta6.b),"variance"=c(beta5.v,beta6.v),"mse"=c(beta5.mse,beta6.mse)) ;com.beta5

com.sigma5 = data.frame("method(sigma)"=c("ML","REML"),"bias"=c(sigma5.b,sigma6.b),"variance"=c(sigma5.v,sigma6.v),"mse"=c(sigma5.mse,sigma6.mse)) ;com.sigma5

com.phi5 = data.frame("method(phi)"=c("ML","REML"),"bias"=c(phi5.b,phi6.b),"variance"=c(phi5.v,phi6.v),"mse"=c(phi5.mse,phi6.mse)) ;com.phi5


```

There is no such difference of estimators obtained by ML with respect to REML. 




















